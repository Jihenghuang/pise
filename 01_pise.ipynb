{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:\n",
    "\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "    \n",
    "        self.x_u = X_u[:,0:1]\n",
    "        self.t_u = X_u[:,1:2]\n",
    "        \n",
    "        self.x_f = X_f[:,0:1]\n",
    "        self.t_f = X_f[:,1:2]\n",
    "        \n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=[None, self.x_u.shape[1]])\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=[None, self.t_u.shape[1]])        \n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])        \n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_u_tf, self.t_u_tf) \n",
    "        self.f_pred = self.net_f(self.x_f_tf, self.t_f_tf)         \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + tf.reduce_mean(tf.square(self.f_pred))\n",
    "         \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "                \n",
    "    def initialize_NN(self, layers):  \n",
    "        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0, num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)  \n",
    "            \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        \n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        \n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        \n",
    "        num_layers = len(weights) + 1\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0         # scaling\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        \n",
    "        return Y\n",
    "            \n",
    "    def net_u(self, x, t):\n",
    "        \n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
    "        u = tf.clip_by_value(u, clip_value_min=0, clip_value_max=0.1)\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x,t):\n",
    "        \n",
    "        u = self.net_u(x,t)\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        f = u_t + 30 * u_x - 600 * u * u_x          # conservation law here\n",
    "        \n",
    "        return f\n",
    "\n",
    "    \n",
    "    def callback(self, loss):\n",
    "        \n",
    "        print('Loss:', loss)\n",
    "        \n",
    "    def train(self):     \n",
    "        \n",
    "        tf_dict = {self.x_u_tf: self.x_u, self.t_u_tf: self.t_u, self.u_tf: self.u,\n",
    "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}                                                                                                                       \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss], \n",
    "                                loss_callback = self.callback)        \n",
    "                                    \n",
    "    def predict(self, X_star):      \n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, {self.x_u_tf: X_star[:,0:1], self.t_u_tf: X_star[:,1:2]})  \n",
    "        f_star = self.sess.run(self.f_pred, {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]})   \n",
    "        \n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9)\n",
    "tf.set_random_seed(9)\n",
    "\n",
    "N_u = 1000\n",
    "N_f = 1\n",
    "layers = [2, 50, 50, 50, 50, 50, 50, 50, 50, 50, 1]\n",
    "\n",
    "data = scipy.io.loadmat('../data/lwr.mat')\n",
    "\n",
    "\n",
    "t = data['tScale'].T.flatten()[:,None]\n",
    "x = data['xScale'].T.flatten()[:,None]\n",
    "Exact = np.real(data['k'])       # density k, Exact has 1440 rows (time), 9 columns (locations)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # X_star shape is 25600 * 2, x left t right\n",
    "u_star = Exact.flatten()[:,None]       # u_star shape is 25600 * 1, flatten to one dimension       \n",
    "\n",
    "# Domain boundaries\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)     \n",
    "\n",
    "\n",
    "xx2 = np.hstack((X[:,1:2], T[:,1:2]))\n",
    "uu2 = Exact[:,1:2]\n",
    "xx3 = np.hstack((X[:,-2:-1], T[:,-2:-1]))\n",
    "uu3 = Exact[:,-2:-1]\n",
    "xx6 = np.hstack((X[:,125:126], T[:,125:126]))\n",
    "uu6 = Exact[:,125:126]\n",
    "xx7 = np.hstack((X[:,250:251], T[:,250:251]))\n",
    "uu7 = Exact[:,250:251]\n",
    "xx8 = np.hstack((X[:,375:376], T[:,375:376]))\n",
    "uu8 = Exact[:,375:376]\n",
    "\n",
    "# X_f_train (1st command) becomes the random numbers between 0 and 1, dimension 10000 * 2\n",
    "# X_f_train then vertically stack the X_u_train after itself, becomes 10456 * 2\n",
    "X_u_train = np.vstack([xx2, xx3, xx6, xx7, xx8])\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)    # Latin-hypercube designs (lhs) function: lhs(n, [samples, criterion, iterations])\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "X_f_train = X_u_train\n",
    "u_train = np.vstack([uu2, uu3, uu6, uu7, uu8])\n",
    "\n",
    "# 100 points from the 456 training data.\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub)\n",
    "\n",
    "start_time = time.time()                \n",
    "model.train()\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred, f_pred = model.predict(X_star) # X_star is the 25600 * 2, x, t coordiates\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "    \n",
    "        self.x_u = X_u[:,0:1]\n",
    "        self.t_u = X_u[:,1:2]\n",
    "        \n",
    "        self.x_f = X_f[:,0:1]\n",
    "        self.t_f = X_f[:,1:2]\n",
    "        \n",
    "        self.u = u\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=[None, self.x_u.shape[1]])\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=[None, self.t_u.shape[1]])        \n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])        \n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_u_tf, self.t_u_tf) \n",
    "        self.f_pred = self.net_f(self.x_f_tf, self.t_f_tf)         \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + 0 * tf.reduce_mean(tf.square(self.f_pred))  \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "                \n",
    "    def initialize_NN(self, layers):  \n",
    "        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0, num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)  \n",
    "            \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        \n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        \n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        \n",
    "        num_layers = len(weights) + 1\n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0         # scaling\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        \n",
    "        return Y\n",
    "            \n",
    "    def net_u(self, x, t):\n",
    "        \n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
    "        u = tf.clip_by_value(u, clip_value_min=0, clip_value_max=0.1)\n",
    "        \n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x,t):\n",
    "        \n",
    "        u = self.net_u(x,t)\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        f = u_t + 30 * u_x - 600 * u * u_x          # conservation law here\n",
    "        \n",
    "        return f\n",
    "\n",
    "    \n",
    "    def callback(self, loss):\n",
    "        \n",
    "        print('Loss:', loss)\n",
    "        \n",
    "    def train(self):     \n",
    "        \n",
    "        tf_dict = {self.x_u_tf: self.x_u, self.t_u_tf: self.t_u, self.u_tf: self.u,\n",
    "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}                                                                                                                       \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss], \n",
    "                                loss_callback = self.callback)        \n",
    "                                    \n",
    "    def predict(self, X_star):      \n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, {self.x_u_tf: X_star[:,0:1], self.t_u_tf: X_star[:,1:2]})  \n",
    "        f_star = self.sess.run(self.f_pred, {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]})   \n",
    "        \n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NN(X_u_train, u_train, X_f_train, layers, lb, ub)\n",
    "\n",
    "start_time = time.time()                \n",
    "model2.train()\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred2, f_pred2 = model2.predict(X_star) # X_star is the 25600 * 2, x, t coordiates\n",
    "\n",
    "error_u2 = np.linalg.norm(u_star-u_pred2,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u2))                     \n",
    "\n",
    "U_pred2 = griddata(X_star, u_pred2.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = newfig(2, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: PIDL: u(t,x) ##################    \n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1, bottom=1 - 8/20, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cax.tick_params(labelsize=20)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', markersize = 2, clip_on = False)\n",
    "\n",
    "ax.set_xlabel('Time $t$ (s)', fontsize = 20)\n",
    "ax.set_ylabel('Location $x$ (m)', fontsize = 20)\n",
    "ax.legend(frameon=False, loc = 'best', fontsize = 20)\n",
    "ax.set_title('PIDL Estimation $\\\\rho (x,t)$ (veh./m)', fontsize = 20)\n",
    "\n",
    "####### Row 1: DL: u(t,x) ##################    \n",
    "gs1 = gridspec.GridSpec(1, 2)\n",
    "gs1.update(top = 8/20, bottom=0, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs1[:, :])\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "h = ax.imshow(U_pred2.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cax.tick_params(labelsize=20)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', markersize = 2, clip_on = False)\n",
    "\n",
    "ax.set_xlabel('Time $t$ (s)', fontsize = 20)\n",
    "ax.set_ylabel('Location $x$ (m)', fontsize = 20)\n",
    "ax.legend(frameon=False, loc = 'best', fontsize = 20)\n",
    "ax.set_title('DL Estimation $\\\\rho (x,t)$ (veh./m)', fontsize = 20)\n",
    "\n",
    "# savefig('fixed_1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
